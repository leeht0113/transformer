{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from models import Transformer\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leeht\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59514"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'bos_token':'<s>'})\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's the most popular sport in your country?</td>\n",
       "      <td>Quel est le sport le plus populaire dans votre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She talked him into buying a new house.</td>\n",
       "      <td>Elle le persuada d'acheter une nouvelle maison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think it's a bad idea.</td>\n",
       "      <td>Je pense que c'est une mauvaise idée.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was hurt and upset.</td>\n",
       "      <td>J'étais blessé et énervé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom already knows the truth.</td>\n",
       "      <td>Tom sait déjà la vérité.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209457</th>\n",
       "      <td>The nurse hit a blood vessel.</td>\n",
       "      <td>L'infirmière a tapé dans une veine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209458</th>\n",
       "      <td>We can't trust Tom anymore.</td>\n",
       "      <td>Nous ne pouvons plus faire confiance à Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209459</th>\n",
       "      <td>I need a pen and paper.</td>\n",
       "      <td>J'ai besoin d'un papier et d'un stylo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209460</th>\n",
       "      <td>You're conceited.</td>\n",
       "      <td>Vous êtes suffisants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209461</th>\n",
       "      <td>I think Tom could be persuaded to help.</td>\n",
       "      <td>Je pense que Tom pourrait être amené à donner ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209462 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   src  \\\n",
       "0       What's the most popular sport in your country?   \n",
       "1              She talked him into buying a new house.   \n",
       "2                             I think it's a bad idea.   \n",
       "3                                I was hurt and upset.   \n",
       "4                         Tom already knows the truth.   \n",
       "...                                                ...   \n",
       "209457                   The nurse hit a blood vessel.   \n",
       "209458                     We can't trust Tom anymore.   \n",
       "209459                         I need a pen and paper.   \n",
       "209460                               You're conceited.   \n",
       "209461         I think Tom could be persuaded to help.   \n",
       "\n",
       "                                                      tar  \n",
       "0       Quel est le sport le plus populaire dans votre...  \n",
       "1         Elle le persuada d'acheter une nouvelle maison.  \n",
       "2                   Je pense que c'est une mauvaise idée.  \n",
       "3                               J'étais blessé et énervé.  \n",
       "4                                Tom sait déjà la vérité.  \n",
       "...                                                   ...  \n",
       "209457                L'infirmière a tapé dans une veine.  \n",
       "209458        Nous ne pouvons plus faire confiance à Tom.  \n",
       "209459             J'ai besoin d'un papier et d'un stylo.  \n",
       "209460                              Vous êtes suffisants.  \n",
       "209461  Je pense que Tom pourrait être amené à donner ...  \n",
       "\n",
       "[209462 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_preprocess.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.src = tokenizer(list(self.data['src']), padding=True, truncation=True, max_length = self.max_len, return_tensors='pt').input_ids\n",
    "        self.tar = tokenizer(['</s>' + s for s in self.data['tar']], padding=True, truncation=True, max_length = self.max_len, return_tensors='pt').input_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tar[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_preprocess.csv')\n",
    "tokenizer.add_special_tokens({'bos_token':'<s>'})\n",
    "custom_ds = CustomDataset(train, tokenizer, 120)\n",
    "train_ds, valid_ds = torch.utils.data.random_split(custom_ds, [0.8, 0.2])\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e2ada242f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54591\n",
      "tensor([ 2942,  2408,   453,     3,     0, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513])\n",
      "move along now.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "tensor([59514,    15, 18619,   643,  3954,   282,     3,     0, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513])\n",
      "<s> avance maintenant.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "idx = train_ds.indices[i]\n",
    "print(idx)\n",
    "src_text, trg_text = custom_ds.__getitem__(idx)\n",
    "print(src_text)\n",
    "print(tokenizer.decode(src_text))\n",
    "print(trg_text)\n",
    "print(tokenizer.decode(trg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59514"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59513"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size + 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 120\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, batch_size, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_embedding): InputEmbeddings(\n",
      "    (embedding): Embedding(59515, 512)\n",
      "  )\n",
      "  (decoder_embedding): InputEmbeddings(\n",
      "    (embedding): Embedding(59515, 512)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (self_attn): MultiHeadAttentionLayer(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttentionLayer(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (cross_attn): MultiHeadAttentionLayer(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=59515, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 81])\n",
      "tensor([   47,   531,     6,    75,    79,    15,  6742,  8095,     3,     0,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513])\n",
      "I don't have a cellphone.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "torch.Size([32, 120])\n",
      "tensor([    0,   131,    81,     6,   211, 10608,     9,     5,   492,  8989,\n",
      "         8095,  6742,  1308,  1819,     3,     0, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513])\n",
      "</s> Je n'ai pas de téléphone cellulaire.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "src_texts, tar_texts = next(iter(train_dl))\n",
    "print(src_texts.shape)\n",
    "print(src_texts[0])\n",
    "print(tokenizer.decode(src_texts[0]))\n",
    "print(tar_texts.shape)\n",
    "print(tar_texts[0])\n",
    "print(tokenizer.decode(tar_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 81])\n",
      "tensor([   47,     6,   122,  4168,   240,   877,     3,     0, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513])\n",
      "I'm trying my best.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "torch.Size([32, 120])\n",
      "tensor([    0,   131,  5901,   900,     5,   505,   251,  7084,     3,     0,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513])\n",
      "</s> Je fais de mon mieux.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "src_texts, tar_texts = next(iter(valid_dl))\n",
    "print(src_texts.shape)\n",
    "print(src_texts[0])\n",
    "print(tokenizer.decode(src_texts[0]))\n",
    "print(tar_texts.shape)\n",
    "print(tar_texts[0])\n",
    "print(tokenizer.decode(tar_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 82])\n",
      "torch.Size([32, 120])\n",
      "torch.Size([32, 82])\n",
      "torch.Size([32, 120])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_texts = src_texts.to(device)\n",
    "tar_texts = tar_texts.to(device)\n",
    "output, attention = model(src_texts, tar_texts[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 119, 59515])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59515"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3808, 59515])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.contiguous().view(-1, vocab_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 119, 82])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3808])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_texts[:, 1:].contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1801, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output.contiguous().view(-1, vocab_size), tar_texts[:, 1:].contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
