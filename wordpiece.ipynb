{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer \n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she ordered him to clean up his room.</td>\n",
       "      <td>elle lui ordonna de nettoyer sa chambre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are you sure you don't want coffee?</td>\n",
       "      <td>êtes-vous certaine de ne pas vouloir de café ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm sorry, i can't come today.</td>\n",
       "      <td>je suis désolé, je ne peux pas venir aujourd'hui.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this isn't my first time riding a bicycle.</td>\n",
       "      <td>ce n'est pas la première fois que je fais du v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>somebody hit me.</td>\n",
       "      <td>j’ai été frappé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209457</th>\n",
       "      <td>i don't want to go.</td>\n",
       "      <td>je ne veux pas y aller.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209458</th>\n",
       "      <td>did you send them?</td>\n",
       "      <td>les avez-vous envoyés ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209459</th>\n",
       "      <td>there is no antidote.</td>\n",
       "      <td>il n'y a pas d'antidote.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209460</th>\n",
       "      <td>i don't think they're married.</td>\n",
       "      <td>je ne pense pas qu'ils soient mariés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209461</th>\n",
       "      <td>i'm sure you won't disappoint me.</td>\n",
       "      <td>je suis certaine que vous ne me décevrez pas.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209462 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               src  \\\n",
       "0            she ordered him to clean up his room.   \n",
       "1              are you sure you don't want coffee?   \n",
       "2                   i'm sorry, i can't come today.   \n",
       "3       this isn't my first time riding a bicycle.   \n",
       "4                                 somebody hit me.   \n",
       "...                                            ...   \n",
       "209457                         i don't want to go.   \n",
       "209458                          did you send them?   \n",
       "209459                       there is no antidote.   \n",
       "209460              i don't think they're married.   \n",
       "209461           i'm sure you won't disappoint me.   \n",
       "\n",
       "                                                      tar  \n",
       "0                elle lui ordonna de nettoyer sa chambre.  \n",
       "1          êtes-vous certaine de ne pas vouloir de café ?  \n",
       "2       je suis désolé, je ne peux pas venir aujourd'hui.  \n",
       "3       ce n'est pas la première fois que je fais du v...  \n",
       "4                                        j’ai été frappé.  \n",
       "...                                                   ...  \n",
       "209457                            je ne veux pas y aller.  \n",
       "209458                            les avez-vous envoyés ?  \n",
       "209459                           il n'y a pas d'antidote.  \n",
       "209460              je ne pense pas qu'ils soient mariés.  \n",
       "209461      je suis certaine que vous ne me décevrez pas.  \n",
       "\n",
       "[209462 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_preprocess.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_df['src']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          5\n",
       "1          8\n",
       "2          7\n",
       "3          7\n",
       "4         10\n",
       "          ..\n",
       "209457     8\n",
       "209458     6\n",
       "209459     8\n",
       "209460     4\n",
       "209461     5\n",
       "Name: src, Length: 209462, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length = train_df['src'].apply(lambda x: len(x.split(' ')))\n",
    "src_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    209462.00000\n",
       "mean          6.08043\n",
       "std           2.48125\n",
       "min           1.00000\n",
       "25%           4.00000\n",
       "50%           6.00000\n",
       "75%           7.00000\n",
       "max          55.00000\n",
       "Name: src, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False, clean_text = True)\n",
    "# eng_tokenizer = Tokenizer(models.WordPiece())\n",
    "# # tokenizer.normalizer = normalizers.NFKC()\n",
    "# # tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "# eng_tokenizer.decoder = decoders.WordPiece()\n",
    "# trainer = trainers.WordPieceTrainer(\n",
    "#     vocab_size = 30000,\n",
    "#     # initial_alphabet = pre_tokenizers.ByteLevel.alphabet(),\n",
    "#     min_frequency=10,\n",
    "#     limit_alphabet=6000,\n",
    "#     show_progress=True,\n",
    "#     special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_tokenizer.train_from_iterator(train_df['src'].values, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 데이터 저장\n",
    "# with open('eng-tokenizer.pkl', 'wb') as f:\n",
    "# \tpickle.dump(eng_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 로드\n",
    "# with open('eng-tokenizer.pkl', 'rb') as f:\n",
    "# \teng_tokenizer = pickle.load(f)\n",
    "\t\n",
    "# eng_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = eng_tokenizer.encode(train_df['src'][50])\n",
    "# print('원본 :', train_df['src'][50])\n",
    "# print('토큰화 결과 :', encoded.tokens)\n",
    "# print('정수 인코딩 :',encoded.ids)\n",
    "# print('디코딩 :',eng_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False, clean_text = True)\n",
    "# fra_tokenizer = Tokenizer(models.WordPiece())\n",
    "# # tokenizer.normalizer = normalizers.NFKC()\n",
    "# # tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "# fra_tokenizer.decoder = decoders.WordPiece()\n",
    "# trainer = trainers.WordPieceTrainer(\n",
    "#     vocab_size = 30000,\n",
    "#     # initial_alphabet = pre_tokenizers.ByteLevel.alphabet(),\n",
    "#     min_frequency=2,\n",
    "#     limit_alphabet=6000,\n",
    "#     show_progress=True,\n",
    "#     special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fra_tokenizer.train_from_iterator(train_df['tar'].values, trainer=trainer, length = len(train_df['tar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 저장\n",
    "# with open('fra-tokenizer.pkl', 'wb') as f:\n",
    "# \tpickle.dump(fra_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 로드\n",
    "# with open('fra-tokenizer.pkl', 'rb') as f:\n",
    "# \tfra_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = fra_tokenizer.encode(train_df['tar'][50])\n",
    "# print('원본 :', train_df['tar'][50])\n",
    "# print('토큰화 결과 :', encoded.tokens)\n",
    "# print('정수 인코딩 :',encoded.ids)\n",
    "# print('디코딩 :',fra_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fra_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./eng-tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'english.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 2\n",
    "\n",
    "eng_tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                show_progress=True,\n",
    "                wordpieces_prefix='##',\n",
    "                # special_tokens = ['<s>', '</s>']\n",
    "                )\n",
    "eng_tokenizer.save_model('./', 'eng-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Their deep love for each other was unequivocal.\n",
      "토큰화 결과 : ['Their', 'deep', 'love', 'for', 'each', 'other', 'was', 'une', '##qu', '##iv', '##ocal', '.']\n",
      "정수 인코딩 : [3073, 2032, 553, 205, 1013, 630, 197, 8287, 882, 1883, 16523, 11]\n",
      "디코딩 : Their deep love for each other was unequivocal.\n"
     ]
    }
   ],
   "source": [
    "encoded = eng_tokenizer.encode(train_df['src'][50])\n",
    "print('원본 :', train_df['src'][50])\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',eng_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fra-tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'fra.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "fra_tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                show_progress=True,\n",
    "                wordpieces_prefix='##',\n",
    "                # special_tokens = ['<s>', '</s>']\n",
    "                )\n",
    "\n",
    "fra_tokenizer.save_model('./', 'fra-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Leur profond amour l'un pour l'autre était sans équivoque.\n",
      "토큰화 결과 : ['Leur', 'profond', 'amour', 'l', \"'\", 'un', 'pour', 'l', \"'\", 'autre', 'était', 'sans', 'équiv', '##oque', '.']\n",
      "정수 인코딩 : [4457, 3778, 2217, 65, 7, 208, 255, 65, 7, 504, 325, 759, 15396, 2414, 13]\n",
      "디코딩 : Leur profond amour l ' un pour l ' autre était sans équivoque.\n"
     ]
    }
   ],
   "source": [
    "encoded = fra_tokenizer.encode(train_df['tar'][50])\n",
    "print('원본 :', train_df['tar'][50])\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',fra_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
