{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer \n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This message is for Tom.</td>\n",
       "      <td>Ce message est pour Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom locked himself in his room and cried.</td>\n",
       "      <td>Tom s'est enfermé dans sa chambre et a pleuré.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought that Tom was in Australia.</td>\n",
       "      <td>Je croyais que Tom était en Australie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don't you think it's a bad thing?</td>\n",
       "      <td>Tu ne penses pas que c'est une mauvaise chose?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I often slept on that bench when I was homeless.</td>\n",
       "      <td>J'ai souvent dormi sur ce banc quand j'étais s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209457</th>\n",
       "      <td>I got a lot of mail this morning.</td>\n",
       "      <td>Ce matin j'ai beaucoup de courrier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209458</th>\n",
       "      <td>What time is your plane landing?</td>\n",
       "      <td>À quelle heure votre avion atterrit-il?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209459</th>\n",
       "      <td>There's so much I want to show you.</td>\n",
       "      <td>Il y a tant que je veuille te montrer !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209460</th>\n",
       "      <td>I want a chair.</td>\n",
       "      <td>Je désire une chaise.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209461</th>\n",
       "      <td>Did you see the show?</td>\n",
       "      <td>As-tu vu le spectacle ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209462 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     src  \\\n",
       "0                               This message is for Tom.   \n",
       "1              Tom locked himself in his room and cried.   \n",
       "2                   I thought that Tom was in Australia.   \n",
       "3                      Don't you think it's a bad thing?   \n",
       "4       I often slept on that bench when I was homeless.   \n",
       "...                                                  ...   \n",
       "209457                 I got a lot of mail this morning.   \n",
       "209458                  What time is your plane landing?   \n",
       "209459               There's so much I want to show you.   \n",
       "209460                                   I want a chair.   \n",
       "209461                             Did you see the show?   \n",
       "\n",
       "                                                      tar  \n",
       "0                                Ce message est pour Tom.  \n",
       "1          Tom s'est enfermé dans sa chambre et a pleuré.  \n",
       "2                  Je croyais que Tom était en Australie.  \n",
       "3          Tu ne penses pas que c'est une mauvaise chose?  \n",
       "4       J'ai souvent dormi sur ce banc quand j'étais s...  \n",
       "...                                                   ...  \n",
       "209457                Ce matin j'ai beaucoup de courrier.  \n",
       "209458            À quelle heure votre avion atterrit-il?  \n",
       "209459            Il y a tant que je veuille te montrer !  \n",
       "209460                              Je désire une chaise.  \n",
       "209461                            As-tu vu le spectacle ?  \n",
       "\n",
       "[209462 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_preprocess.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_df['src']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          5\n",
       "1          8\n",
       "2          7\n",
       "3          7\n",
       "4         10\n",
       "          ..\n",
       "209457     8\n",
       "209458     6\n",
       "209459     8\n",
       "209460     4\n",
       "209461     5\n",
       "Name: src, Length: 209462, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length = train_df['src'].apply(lambda x: len(x.split(' ')))\n",
    "src_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    209462.00000\n",
       "mean          6.08043\n",
       "std           2.48125\n",
       "min           1.00000\n",
       "25%           4.00000\n",
       "50%           6.00000\n",
       "75%           7.00000\n",
       "max          55.00000\n",
       "Name: src, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./eng-tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'english.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                wordpieces_prefix='##',\n",
    "                # special_tokens = ['<s>', '</s>', '<pad>']\n",
    "                )\n",
    "tokenizer.save_model('./', 'eng-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leeht\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leeht\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./eng-tokenizer-vocab.txt', vocab_size=11564, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./eng-tokenizer-vocab.txt', local_files_only=True, lowercase=False, strip_accents=False)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Their deep love for each other was unequivocal.\n",
      "토큰화 결과 : ['their', 'deep', 'love', 'for', 'each', 'other', 'was', 'une', '##qu', '##iv', '##oc', '##al', '.']\n",
      "정수 인코딩 : [2, 828, 2035, 556, 208, 1016, 633, 200, 8290, 885, 1885, 1485, 231, 14, 3]\n",
      "디코딩 : [CLS] their deep love for each other was unequivocal. [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(train_df['src'][50])\n",
    "print('원본 :', train_df['src'][50])\n",
    "print('토큰화 결과 :',tokenizer.tokenize(train_df['src'][50]))\n",
    "print('정수 인코딩 :',encoded)\n",
    "print('디코딩 :',tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[828, 2035, 556, 208, 1016, 633, 200, 8290, 885, 1885, 1486, 231, 14]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(train_df['src'][50], add_special_tokens=False)\n",
    "encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'their deep love for each other was unequivocal.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fra.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_df['tar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fra = BertWordPieceTokenizer(lowercase=True, strip_accents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fra-tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'fra.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer_fra.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency,\n",
    "                wordpieces_prefix='##',\n",
    "                # special_tokens = ['<s>', '</s>', '<pad>']\n",
    "                )\n",
    "tokenizer_fra.save_model('./', 'fra-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./fra-tokenizer-vocab.txt', vocab_size=15182, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./fra-tokenizer-vocab.txt', local_files_only=True, lowercase=False, strip_accents=False)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Leur profond amour l'un pour l'autre était sans équivoque.\n",
      "토큰화 결과 : ['leur', 'profond', 'amour', 'l', \"'\", 'un', 'pour', 'l', \"'\", 'autre', 'était', 'sans', 'équiv', '##oque', '.']\n",
      "정수 인코딩 : [2, 710, 3539, 2078, 42, 10, 157, 185, 42, 10, 443, 272, 659, 14356, 2248, 16, 3]\n",
      "디코딩 : [CLS] leur profond amour l'un pour l'autre était sans équivoque. [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(train_df['tar'][50])\n",
    "print('원본 :', train_df['tar'][50])\n",
    "print('토큰화 결과 :',tokenizer.tokenize(train_df['tar'][50]))\n",
    "print('정수 인코딩 :',encoded)\n",
    "print('디코딩 :',tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
