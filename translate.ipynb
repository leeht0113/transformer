{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from models import Transformer\n",
    "from transformers import AutoTokenizer\n",
    "from train import evaluate\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leeht\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "tokenizer.add_special_tokens({'bos_token':'<s>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): InputEmbeddings(\n",
       "    (embedding): Embedding(59515, 512)\n",
       "  )\n",
       "  (decoder_embedding): InputEmbeddings(\n",
       "    (embedding): Embedding(59515, 512)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttentionLayer(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttentionLayer(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttentionLayer(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=59515, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size + 1\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 120\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "base_directory = './'\n",
    "model = Transformer(tokenizer, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, batch_size, device).to(device)\n",
    "model.load_state_dict(torch.load(base_directory + 'transformers_english_to_french_20.pt', weights_only=True))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You may give the book to whoever wants it.</td>\n",
       "      <td>Tu peux donner le livre à qui en voudra.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We couldn't do that.</td>\n",
       "      <td>Nous ne le pouvions pas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I purchased a new car last year.</td>\n",
       "      <td>J'ai acheté une nouvelle voiture l'année derni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The situation is growing serious.</td>\n",
       "      <td>La situation devient sérieuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom knows everybody.</td>\n",
       "      <td>Tom connaît tout le monde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23269</th>\n",
       "      <td>Ready! Get set! Go!</td>\n",
       "      <td>À vos marques, prêts, partez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23270</th>\n",
       "      <td>Where do I sign?</td>\n",
       "      <td>Où est-ce que je signe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23271</th>\n",
       "      <td>I do this for a living.</td>\n",
       "      <td>Je fais ça pour vivre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23272</th>\n",
       "      <td>We'll shoot.</td>\n",
       "      <td>Nous tirerons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23273</th>\n",
       "      <td>You're not making sense.</td>\n",
       "      <td>Ce que tu dis n'a aucun sens.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23274 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              src  \\\n",
       "0      You may give the book to whoever wants it.   \n",
       "1                            We couldn't do that.   \n",
       "2                I purchased a new car last year.   \n",
       "3               The situation is growing serious.   \n",
       "4                            Tom knows everybody.   \n",
       "...                                           ...   \n",
       "23269                         Ready! Get set! Go!   \n",
       "23270                            Where do I sign?   \n",
       "23271                     I do this for a living.   \n",
       "23272                                We'll shoot.   \n",
       "23273                    You're not making sense.   \n",
       "\n",
       "                                                     tar  \n",
       "0               Tu peux donner le livre à qui en voudra.  \n",
       "1                               Nous ne le pouvions pas.  \n",
       "2      J'ai acheté une nouvelle voiture l'année derni...  \n",
       "3                         La situation devient sérieuse.  \n",
       "4                             Tom connaît tout le monde.  \n",
       "...                                                  ...  \n",
       "23269                     À vos marques, prêts, partez !  \n",
       "23270                            Où est-ce que je signe?  \n",
       "23271                             Je fais ça pour vivre.  \n",
       "23272                                     Nous tirerons.  \n",
       "23273                      Ce que tu dis n'a aucun sens.  \n",
       "\n",
       "[23274 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_preprocess.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.src = self.tokenizer(list(self.data['src']), padding=True, truncation=True, max_length = self.max_len, return_tensors='pt').input_ids\n",
    "        self.tar = self.tokenizer(['<s>' + s for s in self.data['tar']], padding=True, truncation=True, max_length = self.max_len, return_tensors='pt').input_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tar[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f26cc16780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = CustomDataset(test_df, tokenizer, 120)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64)\n",
    "test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 52])\n",
      "tensor([  213,   202,   946,     4,  3006,    12, 29198,  5766,    61,     3,\n",
      "            0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513])\n",
      "You may give the book to whoever wants it.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "torch.Size([64, 101])\n",
      "tensor([59514,   491,   357,  2522,   531,  1432,    19, 10867, 23740,    17,\n",
      "           44,    23,  2175,  9783,   756,     3,     0, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513])\n",
      "<s> Tu peux donner le livre à qui en voudra.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "test_src, test_tar = next(iter(test_dl))\n",
    "print(test_src.shape)\n",
    "print(test_src[0])\n",
    "print(tokenizer.decode(test_src[0]))\n",
    "print(test_tar.shape)\n",
    "print(test_tar[0])\n",
    "print(tokenizer.decode(test_tar[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           False]]]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src_tensor = tokenizer('You may give the book to whoever wants it.', padding=True, truncation=True, max_length = 120, return_tensors='pt').input_ids[0].unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_src_mask(src, tokenizer):\n",
    "    # src: [batch_size, seq_len]\n",
    "    src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    # src_mask: [batch_size, 1, 1, seq_len]\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True, True, True, True, True, True, True, True, True]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_src_mask(src_tensor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59513"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59513, 0]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  213,   202,   946,     4,  3006,    12, 29198,  5766,    61,     3,\n",
       "             0]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, tokenizer, model, device, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_tensor = tokenizer(sentence, padding=True, truncation=True, max_length = max_length, return_tensors='pt').input_ids[0].unsqueeze(0).to(device)\n",
    "        \n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "        src_embedded = model.dropout(model.positional_encoding(model.encoder_embedding(src_tensor)))\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in model.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        enc_output\n",
    "        tar_indexes = [tokenizer.bos_token_id]\n",
    "        for _ in range(max_length):\n",
    "            tar_tensor = torch.LongTensor(tar_indexes).unsqueeze(0).to(device)\n",
    "            # print(tar_tensor)\n",
    "            tar_mask = model.make_tar_mask(tar_tensor)\n",
    "            tar_embedded = model.dropout(model.positional_encoding(model.decoder_embedding(tar_tensor)))\n",
    "            dec_output = tar_embedded\n",
    "            for dec_layer in model.decoder_layers:\n",
    "                dec_output, attention = dec_layer(dec_output, enc_output, src_mask, tar_mask)\n",
    "                output = model.fc(dec_output)\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "            tar_indexes.append(pred_token)\n",
    "            if pred_token == tokenizer.eos_token_id:\n",
    "                # print(pred_token)\n",
    "                break\n",
    "    print(tar_indexes)\n",
    "    tar_tokens = tokenizer.decode(tar_indexes[1:-1])\n",
    "    return tar_tokens, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59514: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59514, 2091, 376, 2903, 6, 93, 531, 772, 17, 6790, 1054, 531, 1432, 8, 10867, 23740, 17, 66, 29, 19, 10867, 23740, 6790, 1672, 21, 501, 146, 1483, 17, 44, 19, 1106, 51, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "tar_tokens, attention = translate(\"You may give the book to whoever wants it.\", tokenizer, model, device, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quelqu'un donne à peine donner la livre à ce que le livre peut-être savoir à qui le dise.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  213,   202,   946,     4,  3006,    12, 29198,  5766,    61,     3,\n",
       "             0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59514,   491,   357,  2522,   531,  1432,    19, 10867, 23740,    17,\n",
       "            44,    23,  2175,  9783,   756,     3,     0, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tar[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, _ = model(test_src[0].unsqueeze(0).to(device), test_tar[0].unsqueeze(0)[:, :-1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tu peux donner le livre à qui le voeuxrai.</s> </s> eeeeeeeeeeeeeeeeeeee.....ee.......</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.argmax(2)[:, :-1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu peux donner le livre à qui en voudra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.argmax(2)[:, -1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bfc512a3464a32bdcda76ca21bf06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid batch iteration:   0%|          | 0/364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.036 | Test PPL: 2.817\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "test_loss = evaluate(model, test_dl, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 23, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
